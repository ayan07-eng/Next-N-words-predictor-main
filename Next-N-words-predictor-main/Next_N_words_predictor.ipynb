{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Next_N_words_predictor.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9weHMmyd8fnq"
      },
      "source": [
        "## **Reading a Raw Text Corpus**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmSy_LOK2aGQ"
      },
      "source": [
        "Retrieve & save raw corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rku6rV2ORpZA"
      },
      "source": [
        "\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "#from Ipython.display import HTML\n",
        "\n",
        "r=urllib.request.urlopen(\"https://www.gutenberg.org/files/730/730-0.txt\").read()  #extracting data from webapge\n",
        "soup=BeautifulSoup(r,\"lxml\")\n",
        "data= str(soup)\n",
        "start=data.find('CHAPTER I')\n",
        "end = data.find('*** END OF THE PROJECT GUTENBERG EBOOK OLIVER TWIST ***')\n",
        "parsed = data[start:end]\n",
        "file = open(\"rawCorpus.txt\",'w')  \n",
        "file.write(parsed)\n",
        "file.flush() \n",
        "file.close()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KZIOy0Y2hzQ"
      },
      "source": [
        "Read the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsdBJa_l2l7g"
      },
      "source": [
        "# Read the corpus from rawCorpus.txt, in a variable `rawReadCorpus`\n",
        "# *** Write code ***\n",
        "file=open(\"rawCorpus.txt\",'r')\n",
        "rawReadCorpus=file.read()\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utKtZeHq4N98"
      },
      "source": [
        "## **Preprocessing the corpus**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2g7eO4Dm4jIn",
        "outputId": "e4674636-119e-4ea4-de6f-cc223e5e0fe5"
      },
      "source": [
        "# Importing modules\n",
        "import nltk\n",
        "nltk.download('punkt') # For tokenizers\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize,RegexpTokenizer,TweetTokenizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWIzYXyz9Zt_",
        "outputId": "31b982f7-ef44-489b-ba13-e75c6003779a"
      },
      "source": [
        "# *** Write code for preprocessing the corpus ***\n",
        "lowercorp=rawReadCorpus\n",
        "lowernew=lowercorp.replace('_','') #handling the edge case of _will_ , etc\n",
        "lowerforword=lowernew.lower()\n",
        "token=RegexpTokenizer(\"[a-z'a-z]+\")  # handling the edge case of can't,won't etc\n",
        "wordtok=token.tokenize(lowerforword)\n",
        "\n",
        "# Print first 5 words/tokens of your preprocessed corpus *** Write code ***\n",
        "print(wordtok[0:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['chapter', 'i', 'treats', 'of', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyG0g3oSADmV",
        "outputId": "d8c5ec17-ae76-400c-e431-aa5ea9449b21"
      },
      "source": [
        "# Importing modules\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5RiDR7TJjKX"
      },
      "source": [
        "## **Language Modeling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlPXGvVaR-ka"
      },
      "source": [
        "from nltk.util import ngrams\n",
        "from nltk.probability import FreqDist\n",
        "unigrams=[]\n",
        "bigrams=[]\n",
        "trigrams=[]\n",
        "agram=ngrams(wordtok,1) #making unigram\n",
        "bgram=ngrams(wordtok,2) #making bigram\n",
        "cgram=ngrams(wordtok,3) #making trigram\n",
        "for content in agram: # *** Write code ***\n",
        "    unigrams.append(content) \n",
        "    # *** Write code ***\n",
        "for content in bgram: # *** Write code ***\n",
        "    bigrams.append(content)\n",
        "for content in cgram: # *** Write code ***\n",
        "    trigrams.append(content)\n",
        "\n",
        "arti=[\"a\",\"an\",\"the\",\"to\",\"in\",\"into\",\"on\",\"onto\",\"at\",\"inside\",\"over\",\"above\",\"below\",\"beneath\",\"under\",\"underneath\",\"by\",\"near\",\"between\",\"among\",\"opposite\",\"across\",\"along\",\"behind\",\"off\",\"toward\",'within',\"this\",\"that\",\"these\",\"those\",\"my\",\"your\",\"his\",\"her\",\"its\",\"our\",\"their\",\"much\",\"many\",\"few\",\"little\",\"most\",\"some\",\"any\",\"enough\",\"either\",\"neither\",\"each\",\"every\",\"half\",\"such\",\"other\",\"another\"]+stopwords.words('english')\n",
        "unigrams_Processed = [gram for gram in unigrams if not any(stop in gram for stop in arti)] # arti is unwanted words\n",
        "bigrams_Processed =[gram for gram in bigrams if not any(stop in gram for stop in arti)]\n",
        "trigrams_Processed =[gram for gram in trigrams if not any(stop in gram for stop in arti)]\n",
        "\n",
        "def get_ngrams_freqDist(n, ngramList):\n",
        "    #This function computes the frequency corresponding to each ngram in ngramList \n",
        "    #Here, n=1 for unigram, n=2 for bigram, etc.\n",
        "    #ngramList = list of unigrams when n=1, ngramList = list of bigrams when n=2\n",
        "    #Returns: ngram_freq_dict (a Python dictionary where key = a ngram, value = its frequency)\n",
        "    \n",
        "    # *** Write code ***\n",
        "    ngram_freq_dict=FreqDist()  # making frequncy distribution \n",
        "    for t in ngramList:\n",
        "      ngram_freq_dict[t]+=1\n",
        "    \n",
        "    return ngram_freq_dict\n",
        "                                               \n",
        "unigrams_freqDist = get_ngrams_freqDist(1, unigrams)\n",
        "unigrams_Processed_freqDist = get_ngrams_freqDist(1, unigrams_Processed)\n",
        "bigrams_freqDist = get_ngrams_freqDist(2, bigrams_Processed)\n",
        "bigrams_Processed_freqDist = get_ngrams_freqDist(2, bigrams_Processed)\n",
        "trigrams_freqDist = get_ngrams_freqDist(3, trigrams_Processed)\n",
        "trigrams_Processed_freqDist = get_ngrams_freqDist(3, trigrams_Processed) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqu8nVV7NREo"
      },
      "source": [
        "## **Next n words' Prediction using Smoothed Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2vnIM26b2WA"
      },
      "source": [
        "For a bigram model, add-one smoothing is defined by $P_{Add-1}(w_i|w_{i-1})=\\frac{count(w_{i-1},w_i)+1}{count(w_{i-1})+V}$.\n",
        "That is, pretend we saw each word one more time than we did.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAGB1_S8NThy"
      },
      "source": [
        "testSent1 = \"There was a sudden jerk, a terrific convulsion of the limbs; and there he\"\n",
        "testSent2 = \"They made room for the stranger, but he sat down\"\n",
        "testSent3 = \"The hungry and destitute situation of the infant orphan was duly reported by\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yLY1ymH-ZuJu"
      },
      "source": [
        "# *** Write code ***\n",
        "ngrams_vocab = {1: set([]), 2: set([]), 3: set([])} # contains unique ngrams. key 1 is for unigrams, key 2 is for bigrams and so on\n",
        "for gram in unigrams:\n",
        "  if gram not in ngrams_vocab[1]:\n",
        "    ngrams_vocab[1].add(gram)\n",
        "\n",
        "for gram in bigrams:\n",
        "  if gram not in ngrams_vocab[2]:\n",
        "    ngrams_vocab[2].add(gram)\n",
        "\n",
        "for gram in trigrams:\n",
        "  if gram not in ngrams_vocab[3]:\n",
        "    ngrams_vocab[3].add(gram)\n",
        "\n",
        "\n",
        "total_ngrams = {1: len(unigrams), 2: len(bigrams), 3: len(trigrams)}   #keep count of total n grams. similar logic for keys as as ngrams_vocab\n",
        "total_vocab = {1: len(ngrams_vocab[1]), 2: len(ngrams_vocab[2]), 3: len(ngrams_vocab[3])}   #keep count of unique n grams \n",
        "\n",
        "ngrams_all_prob = {1: [], 2: [], 3: []}      # This list will contain probability in sorted order\n",
        "\n",
        "for ngram in ngrams_vocab[1]:  \n",
        "  tmplist = [ngram]\n",
        "  tmplist.append(unigrams.count(ngram))\n",
        "  (ngrams_all_prob[1]).append(tmplist)\n",
        "\n",
        "for ngram in ngrams_vocab[2]:\n",
        "  tmplist = [ngram]\n",
        "  tmplist.append(bigrams.count(ngram))\n",
        "  (ngrams_all_prob[2]).append(tmplist)\n",
        "\n",
        "for ngram in ngrams_vocab[3]:\n",
        "  tmplist = [ngram]\n",
        "  tmplist.append(trigrams.count(ngram))\n",
        "  (ngrams_all_prob[3]).append(tmplist)\n",
        "\n",
        "for i in range(3):              # smoothing\n",
        "    for ngram in ngrams_all_prob[i + 1]:\n",
        "        ngram[-1] = (ngram[-1] + 1) / (total_ngrams[i + 1] + total_vocab[i + 1])\n",
        "\n",
        "def Sort_prob(lst):       # returns a list of ngrams sorted by their probability values\n",
        "    return(sorted(lst, key = lambda x: x[1], reverse = True))\n",
        "\n",
        "for i in range(3):      # sorting according to probability values \n",
        "    ngrams_all_prob[i + 1] = Sort_prob(ngrams_all_prob[i + 1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0fNTRuZZAQY",
        "outputId": "60f87bd0-2bba-44d0-de9b-b1cce2303d1f"
      },
      "source": [
        "for i in range(5):\n",
        "  print(ngrams_all_prob[2][i]) # my name is satish  temp - [satish,the,said]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('of', 'the'), 0.0036570311479623007]\n",
            "[('in', 'the'), 0.002557393901858429]\n",
            "[('to', 'the'), 0.0019759765073667267]\n",
            "[('said', 'the'), 0.001908565794961892]\n",
            "[('mr', 'bumble'), 0.001470396164330464]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfoaBZvrN6XS"
      },
      "source": [
        "#predicting next word in given test sentences\n",
        "\n",
        "def predict_by_bigram(t1, n):\n",
        "    temp = []\n",
        "    temp.append(t1[-1])\n",
        "    for j in range(n):  # this for loop will search for next possible words\n",
        "        last_word = temp[-1]\n",
        "        for pred in ngrams_all_prob[2]:\n",
        "            if pred[0][0] == last_word:  # searching for bigram\n",
        "                temp.append(pred[0][1])\n",
        "                break\n",
        "    predicted_tok_sent1 = t1\n",
        "    # print(len(temp))\n",
        "    for i in range(n):\n",
        "        predicted_tok_sent1.append(temp[1 + i])  # appending the predicted words in the tokens\n",
        "\n",
        "    final_predicted = \"\"\n",
        "    for w in predicted_tok_sent1:  # making the string from tokens\n",
        "        final_predicted += w + \" \"\n",
        "    return final_predicted\n",
        "\n",
        "def predict_by_trigram(t2, n):\n",
        "    temp = []\n",
        "    temp.append(t2[-2])\n",
        "    temp.append(t2[-1])\n",
        "    for j in range(n):  # this for loop will search for next possible words\n",
        "        last_word = temp[-1]\n",
        "        second_last_word = temp[-2]\n",
        "        for pred in ngrams_all_prob[3]:\n",
        "            if pred[0][0] == second_last_word and pred[0][1] == last_word:  # searching for trigram\n",
        "                temp.append(pred[0][2])\n",
        "                break\n",
        "    predicted_tok_sent2 = t2\n",
        "    for i in range(n):\n",
        "        predicted_tok_sent2.append(temp[2 + i])  # appending the predicted words in the tokens\n",
        "\n",
        "    final_predicted = \"\"\n",
        "    for w in predicted_tok_sent2:  # making the string from tokens\n",
        "        final_predicted += w + \" \"\n",
        "    return final_predicted\n",
        "\n",
        "def inputsent():\n",
        "    senttence = input (\"enter the sentence\\n\")\n",
        "    n = int(input (\"enter the number of words you want to predict\\n\"))\n",
        "\n",
        "    tok_sent1 = word_tokenize(senttence)  # tokeninzing the input string\n",
        "\n",
        "    ngrams_sent1 = {1: [], 2: [], 3: []}  # contains ngrams formed at the end of testSent1\n",
        "\n",
        "    for i in range(3):\n",
        "      try:\n",
        "        ngrams_sent1[i + 1] = list(ngrams(tok_sent1, i + 1))[-1]\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "    predict_sent1 = {2: [], 3: []}  # will contain bi and trigram prediction\n",
        "\n",
        "    ngrams_sent1_2 = ngrams_sent1[2]\n",
        "    ngrams_sent1_3 = ngrams_sent1[3]\n",
        "\n",
        "    print(\"The predicted words for the given sentence are as follows:\")\n",
        "    print(\"Bigram--->\" + predict_by_bigram(tok_sent1.copy(),n))\n",
        "    try:\n",
        "      print(\"Trigram--->\" + predict_by_trigram(tok_sent1.copy(),n) + '\\n')\n",
        "    except:\n",
        "      print(\"Trgiram model cannot predict on the sentence with words less than 2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dumEJBaeYs2"
      },
      "source": [
        "# perplexity\n",
        "def bigram_perplexity(sentence):\n",
        "    prob = 1\n",
        "    perp = None\n",
        "    words = sentence.split()\n",
        "    ct=0\n",
        "      \n",
        "    for bigram in ngrams(word_tokenize(sentence),2):\n",
        "        #occurence in bigram\n",
        "        if bigram not in bigrams_freqDist.keys():\n",
        "            ct_bigram = 0\n",
        "        else:\n",
        "            ct_bigram = bigrams_freqDist[bigram]\n",
        "        #occurence in unigram     \n",
        "        if bigram[0] not in unigrams_freqDist.keys():\n",
        "            ct_unigram = 0\n",
        "        else:\n",
        "            ct_unigram = unigrams_freqDist[bigram[0]]\n",
        "        prob = prob * ((ct_bigram + 1) / (ct_unigram + len(unigrams_freqDist)))   \n",
        "      \n",
        "    for i in ngrams(word_tokenize(sentence),2) :\n",
        "        ct = ct + 1\n",
        "    perp = (1/prob) ** (1/(ct))\n",
        "    return perp\n",
        "\n",
        "def trigram_perplexity(sentence):\n",
        "    prob = 1\n",
        "    perp = None\n",
        "    words = sentence.split()\n",
        "    ct=0\n",
        "    \n",
        "    for trigram in ngrams(word_tokenize(sentence),3):\n",
        "        #occurence in trigram\n",
        "        if trigram not in trigrams_freqDist.keys():\n",
        "            ct_trigram = 0\n",
        "        else:\n",
        "            ct_trigram = trigrams_freqDist[trigram]\n",
        "        #occurence in bigram\n",
        "        bigram=tuple((trigram[0], trigram[1]))\n",
        "        if bigram not in bigrams_freqDist.keys():\n",
        "            ct_bigram = 0\n",
        "        else:\n",
        "            ct_bigram = bigrams_freqDist[bigram]\n",
        "        prob = prob * ((ct_trigram + 1) / (ct_bigram + len(unigrams_freqDist)))\n",
        "      \n",
        "    for i in ngrams(word_tokenize(sentence),2) :\n",
        "        ct = ct + 1\n",
        "    perp = (1/prob) ** (1/(ct))\n",
        "    return perp\n",
        "def perplexity(sentence):\n",
        "  print(\"the perplexity is as follows:\\n\")\n",
        "  print(\"bigram perplexity =\",bigram_perplexity(predict_by_bigram(tok_sent1.copy(),int(n))))\n",
        "  print(\"trigram perplexity =\",trigram_perplexity(predict_by_trigram(tok_sent1.copy(),int(n))))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "id": "HWe8mpEaZv8k",
        "outputId": "c37694cc-0462-4a91-dc0d-1413ecabbe80"
      },
      "source": [
        "inputsent()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-9b639d40f28e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minputsent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-e9623ba95adc>\u001b[0m in \u001b[0;36minputsent\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minputsent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0msenttence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"enter the sentence\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"enter the number of words you want to predict\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}
